Title         : 统计学习笔记
Author        : Stanley Wu
Email         : bleedtodry@hotmail.com
Affiliation   : GmaxInfo
Logo          : True


~Body: font-family="SimSun"


[TITLE]

[TOC]
# 线性代数
## 线性方程组
给定线性方程组：
~ Equation
a_1 x_1+a_2 x_2+\cdots+a_n x_n = 0
~
如果方程的解为$a_1=\cdots=a_k=0$，则称${x_1,\cdots,x_n}$为线性无关，反之则称为线性相关。
将上述方程改写为：
~ Equation
Ax=0
~
$x$线性无关时，$A$可逆  $x$线性相关时，$A$奇异
## 可逆矩阵
对于矩阵$A$，若存在矩阵$B$，满足$AB=BA=I$，则称$A$是可逆的，称$B$是$A$的逆矩阵，
记作$A^{-1}$。不可逆矩阵也称为奇异矩阵，可逆矩阵称为非奇异矩阵。若$A$可逆，则$Ax=b$
有唯一解$x=A^{-1}b$

# 线性模型
## 特征选择
### Subset Selection
#### Best Subset Selection

### Shrinkage
#### Ridge Regression
~ Equation
\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_j x_{ij})^2+\lambda \sum_{j=1}^{p}\beta_j^2
~
### Dimension Reduction
# 分类算法
## Logistic Regression
基本模型：
~Equation
p(X)=\frac{e^{\beta_0+\beta_{1X}}}{1+e^{\beta_0+\beta_{1X}}}
~
Log-odds logit：
~Equation
\log(\frac{p(X)}{1-p(X)}) = \beta_0+\beta_{1X}
~
最大似然参数估计：
~Equation
  \zeta(\beta_0,\beta_1)= \prod_{i:y_i=1}p(x_i)\prod_{i:y_i=0}(1-p(x_i))
~
##  Linear Discriminant Analysis
Logistic Regression适用于二元离散回归，当因变量大于2个时，使用LDA
# 重抽样
## Cross-Validation
### Leave-One-Out Cross-Validation
![LOOCV]

[LOOCV]: images/LOOCV.png "LOOCV" { width:auto; max-width:80%}
~Equation
  CV=\frac{1}{n}\sum_{i=1}^{n}MSE_i
~
### k-Folder Cross-Validation
~ Equation
CV_{(K)}=\sum_{k=1}^{K}\frac{n_k}{n}MSE_k
~
LOOCV是k-Folder CV的一种特殊情况，即$k=n$。
## Bootstrap
![Bootstrap]

[Bootstrap]: images/Bootstrap.png "Bootstrap" { width:auto; max-width:70% }

# 高维矩阵估计
